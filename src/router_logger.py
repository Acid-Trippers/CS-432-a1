import os
import sys
import json
import time
import subprocess
import httpx
from datetime import datetime

# --- Paths ---
# We use absolute paths here to ensure it works no matter where you run it from
scriptDir = os.path.dirname(os.path.abspath(__file__))
dataDir = os.path.join(scriptDir, '..', 'data')
externalDir = os.path.join(scriptDir, '..', 'external')

classificationFile = os.path.join(dataDir, 'field_metadata.json')
sqlOutputFile = os.path.join(dataDir, 'sql_records.json')
mongoOutputFile = os.path.join(dataDir, 'mongo_records.json')
routerLogFile = os.path.join(dataDir, 'router_logger.txt')

def loadClassificationMap():
    """Loads the rules generated by your classifier."""
    if not os.path.exists(classificationFile):
        print(f"Error: Classification file not found at {classificationFile}")
        print("Run 'python main.py initialise' first.")
        sys.exit(1)

    with open(classificationFile, 'r') as f:
        results = json.load(f)
    
    return {item['fieldName']: item['decision'] for item in results}

def waitForServer(url: str, timeout: int = 15):
    startTime = time.time()
    while time.time() - startTime < timeout:
        try:
            with httpx.Client() as client:
                response = client.get("http://127.0.0.1:8000/")
                if response.status_code == 200:
                    return True
        except (httpx.RequestError, httpx.ConnectError):
            time.sleep(1)
    return False

def appendJsonRecords(filepath, newRecords):
    """Safely appends new records to an existing JSON list file."""
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if not isinstance(data, list):
                    data = []
        except:
            data = []
    else:
        data = []
    
    data.extend(newRecords)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)

def route_record(record, schemaMap, logFile):
    """Helper logic to route a single record and write to the open log file."""
    # Ensure timestamp exists
    if 'sys_ingested_time' not in record:
        record['sys_ingested_time'] = datetime.now().isoformat()
    
    ingestTime = record['sys_ingested_time']
    
    sqlDoc = {}
    mongoDoc = {}

    # Log Header
    logEntry = f"Record received at {ingestTime}\n"
    logEntry += f"{len(record)} Fields\n"

    for field, value in record.items():
        # Default to MONGO if we've never seen this field before
        decision = schemaMap.get(field, "MONGO")

        logEntry += f"{field} : {decision}\n"

        if decision == "SQL":
            sqlDoc[field] = value
        elif decision == "MONGO":
            mongoDoc[field] = value
        elif decision == "BOTH":
            sqlDoc[field] = value
            mongoDoc[field] = value
    
    logEntry += "\n"
    logFile.write(logEntry)

    return sqlDoc, mongoDoc

# --- MODE 1: Batch Processing (For Initialization) ---
def processBatch(sourceFile: str):
    schemaMap = loadClassificationMap()
    
    if not os.path.exists(sourceFile):
        print(f"Error: Source file {sourceFile} not found.")
        return

    with open(sourceFile, 'r', encoding='utf-8') as f:
        records = json.load(f)

    sqlRecords = []
    mongoRecords = []

    # Open log file in Append mode
    with open(routerLogFile, 'a', encoding='utf-8') as logFile:
        
        # Write Start Separator
        logFile.write("\n" + ("-" * 60) + "\n")
        logFile.write(f"INITIALIZATION BATCH: Routing {len(records)} records\n")
        logFile.write(("-" * 60) + "\n\n")

        for record in records:
            sDoc, mDoc = route_record(record, schemaMap, logFile)
            if sDoc: sqlRecords.append(sDoc)
            if mDoc: mongoRecords.append(mDoc)

        # Write End Separator
        logFile.write(("-" * 60) + "\n")
        logFile.write("END OF INITIALIZATION\n")
        logFile.write(("-" * 60) + "\n\n")

    # Append to JSONs
    appendJsonRecords(sqlOutputFile, sqlRecords)
    appendJsonRecords(mongoOutputFile, mongoRecords)

    print(f"Batch routed: {len(sqlRecords)} SQL records, {len(mongoRecords)} Mongo records.")
    print(f"Batch logs appended to {routerLogFile}")

# --- MODE 2: Stream Processing (For Router Command) ---
def processAndSplit(recordCount: int):
    schemaMap = loadClassificationMap()
    
    serverProc = subprocess.Popen(
        [sys.executable, "-m", "uvicorn", "simulation_code:app", "--port", "8000"],
        cwd=externalDir,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL
    )

    url = f"http://127.0.0.1:8000/record/{recordCount}"
    sqlRecords = []
    mongoRecords = []

    try:
        if not waitForServer(url):
            print("Server failed to start.")
            return

        with open(routerLogFile, 'a', encoding='utf-8') as logFile, httpx.stream("GET", url, timeout=None) as response:
            count = 0
            for line in response.iter_lines():
                if not line.startswith("data: "):
                    continue
                
                recordJson = json.loads(line[6:])
                count += 1
                
                sDoc, mDoc = route_record(recordJson, schemaMap, logFile)
                if sDoc: sqlRecords.append(sDoc)
                if mDoc: mongoRecords.append(mDoc)

                if count >= recordCount:
                    break
        
    except Exception as e:
        print(f"Error: {e}")
    finally:
        serverProc.terminate()
        serverProc.wait()

    appendJsonRecords(sqlOutputFile, sqlRecords)
    appendJsonRecords(mongoOutputFile, mongoRecords)

    print(f"Appended {len(sqlRecords)} SQL records and {len(mongoRecords)} Mongo records.")
    print(f"Router logs appended to {routerLogFile}")

if __name__ == "__main__":
    count = 10
    if len(sys.argv) > 1:
        try:
            count = int(sys.argv[1])
        except ValueError:
            pass
    processAndSplit(count)