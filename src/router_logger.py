import os
import sys
import json
import time
import subprocess
import httpx
from datetime import datetime

# --- Paths ---
scriptDir = os.path.dirname(os.path.abspath(__file__))
dataDir = os.path.join(scriptDir, '..', 'data')
externalDir = os.path.join(scriptDir, '..', 'external')

classificationFile = os.path.join(dataDir, 'field_metadata.json')
analyzedFile = os.path.join(dataDir, 'analyzed_data.json')
sqlOutputFile = os.path.join(dataDir, 'sql_records.json')
mongoOutputFile = os.path.join(dataDir, 'mongo_records.json')
routerLogFile = os.path.join(dataDir, 'router_logger.txt')
driftLogFile = os.path.join(dataDir, 'drift_logger.txt')

def loadClassificationMap():
    """Loads the rules generated by your classifier."""
    if not os.path.exists(classificationFile):
        print(f"Error: Classification file not found at {classificationFile}")
        print("Run 'python main.py initialise' first.")
        sys.exit(1)

    with open(classificationFile, 'r') as f:
        results = json.load(f)
    
    return {item['fieldName']: item['decision'] for item in results}

def loadAnalyzedSchema():
    """Loads the dominant types for drift detection."""
    if not os.path.exists(analyzedFile):
        return {}
    
    with open(analyzedFile, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return { item['field_name']: item['dominant_type'] for item in data.get('fields', []) }

def getValType(val):
    """Maps Python types to the schema string types."""
    if val is None: return "null"
    if isinstance(val, bool): return "boolean"
    if isinstance(val, int): return "integer"
    if isinstance(val, float): return "float"
    if isinstance(val, str): return "string"
    if isinstance(val, list): return "array"
    if isinstance(val, dict): return "object"
    return "unknown"

def waitForServer(url: str, timeout: int = 15):
    startTime = time.time()
    while time.time() - startTime < timeout:
        try:
            with httpx.Client() as client:
                response = client.get("http://127.0.0.1:8000/")
                if response.status_code == 200:
                    return True
        except (httpx.RequestError, httpx.ConnectError):
            time.sleep(1)
    return False

def appendJsonRecords(filepath, newRecords):
    """Safely appends new records to an existing JSON list file."""
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if not isinstance(data, list):
                    data = []
        except:
            data = []
    else:
        data = []
    
    data.extend(newRecords)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2)

def update_metadata_file(field_name, new_decision, reason):
    """Updates field_metadata.json with the new decision to handle drift persistently."""
    if not os.path.exists(classificationFile):
        return

    try:
        with open(classificationFile, 'r') as f:
            rules = json.load(f)
        
        updated = False
        for rule in rules:
            if rule['fieldName'] == field_name:
                if rule['decision'] != new_decision:
                    rule['decision'] = new_decision
                    rule['reason'] = reason
                    if "DRIFT_DETECTED" not in rule['flags']:
                        rule['flags'].append("DRIFT_DETECTED")
                    updated = True
                break
        
        if updated:
            with open(classificationFile, 'w') as f:
                json.dump(rules, f, indent=2)
                
    except Exception as e:
        print(f"Error updating metadata: {e}")

def route_record(record, schemaMap, analyzedSchema, logFile):
    """Helper logic to route a single record, check for drift, and write to logs."""
    if 'sys_ingested_time' not in record:
        record['sys_ingested_time'] = datetime.now().isoformat()
    
    ingestTime = record['sys_ingested_time']
    
    sqlDoc = {}
    mongoDoc = {}

    logEntry = f"Record received at {ingestTime}\n"
    logEntry += f"{len(record)} Fields\n"

    for field, value in record.items():
        # 1. Get current decision status
        # Default to MONGO if we've never seen this field before
        existing_decision = schemaMap.get(field, "MONGO")

        # 2. Drift Detection
        expectedType = analyzedSchema.get(field)
        currentType = getValType(value)
        
        # Check for Drift ONLY if it's currently going to SQL (or BOTH).
        # If it is already MONGO, we ignore type changes (Mongo-to-Mongo is safe).
        if expectedType and currentType != expectedType and existing_decision != "MONGO":
            
            # A. Log the Drift
            with open(driftLogFile, 'a', encoding='utf-8') as df:
                df.write(f"Drift detected for field '{field}' at {ingestTime}\n")
                df.write(f"Expected: {expectedType}, Found: {currentType}. Routing to MONGO.\n\n")
            
            # B. Update Metadata (Persistence)
            update_metadata_file(field, "MONGO", f"Drift: Expected {expectedType}, Got {currentType}")
            
            # Update in-memory map so we don't log this again for the rest of the batch
            schemaMap[field] = "MONGO"
            
            decision = "MONGO"
        else:
            # No drift or already Mongo
            decision = existing_decision

        logEntry += f"{field} : {decision}\n"

        if decision == "SQL":
            sqlDoc[field] = value
        elif decision == "MONGO":
            mongoDoc[field] = value
        elif decision == "BOTH":
            sqlDoc[field] = value
            mongoDoc[field] = value
    
    logEntry += "\n"
    logFile.write(logEntry)

    return sqlDoc, mongoDoc

# --- MODE 1: Batch Processing (For Initialization) ---
def processBatch(sourceFile: str):
    schemaMap = loadClassificationMap()
    analyzedSchema = loadAnalyzedSchema()
    
    if not os.path.exists(sourceFile):
        print(f"Error: Source file {sourceFile} not found.")
        return

    with open(sourceFile, 'r', encoding='utf-8') as f:
        records = json.load(f)

    sqlRecords = []
    mongoRecords = []

    with open(routerLogFile, 'a', encoding='utf-8') as logFile:
        logFile.write("\n" + ("-" * 60) + "\n")
        logFile.write(f"INITIALIZATION BATCH: Routing {len(records)} records\n")
        logFile.write(("-" * 60) + "\n\n")

        for record in records:
            sDoc, mDoc = route_record(record, schemaMap, analyzedSchema, logFile)
            if sDoc: sqlRecords.append(sDoc)
            if mDoc: mongoRecords.append(mDoc)

        logFile.write(("-" * 60) + "\n")
        logFile.write("END OF INITIALIZATION\n")
        logFile.write(("-" * 60) + "\n\n")

    appendJsonRecords(sqlOutputFile, sqlRecords)
    appendJsonRecords(mongoOutputFile, mongoRecords)

    print(f"Batch routed: {len(sqlRecords)} SQL records, {len(mongoRecords)} Mongo records.")
    print(f"Batch logs appended to {routerLogFile}")

# --- MODE 2: Stream Processing (For Router Command) ---
def processAndSplit(recordCount: int):
    schemaMap = loadClassificationMap()
    analyzedSchema = loadAnalyzedSchema()
    
    serverProc = subprocess.Popen(
        [sys.executable, "-m", "uvicorn", "simulation_code:app", "--port", "8000"],
        cwd=externalDir,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL
    )

    url = f"http://127.0.0.1:8000/record/{recordCount}"
    sqlRecords = []
    mongoRecords = []

    try:
        if not waitForServer(url):
            print("Server failed to start.")
            return

        with open(routerLogFile, 'a', encoding='utf-8') as logFile, httpx.stream("GET", url, timeout=None) as response:
            count = 0
            for line in response.iter_lines():
                if not line.startswith("data: "):
                    continue
                
                recordJson = json.loads(line[6:])
                count += 1
                
                sDoc, mDoc = route_record(recordJson, schemaMap, analyzedSchema, logFile)
                if sDoc: sqlRecords.append(sDoc)
                if mDoc: mongoRecords.append(mDoc)

                if count >= recordCount:
                    break
        
    except Exception as e:
        print(f"Error: {e}")
    finally:
        serverProc.terminate()
        serverProc.wait()

    appendJsonRecords(sqlOutputFile, sqlRecords)
    appendJsonRecords(mongoOutputFile, mongoRecords)

    print(f"Appended {len(sqlRecords)} SQL records and {len(mongoRecords)} Mongo records.")
    print(f"Router logs appended to {routerLogFile}")

if __name__ == "__main__":
    count = 10
    if len(sys.argv) > 1:
        try:
            count = int(sys.argv[1])
        except ValueError:
            pass
    processAndSplit(count)